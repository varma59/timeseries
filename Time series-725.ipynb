{"cells":[{"cell_type":"markdown","source":["# Time Series Forecasting Model\n","\n","In this notebook, we'll demonstrate an end-to-end data science workflow for a time series forecasting model. In this scenario we will predict the total monthly sales of property in NYC based on the historic sales.\n","\n","Time series forecasting is a common and important problem in business. The tools and methods in this notebook can be applied to other forecasting tasks, such as capacity planning or price forecasting.\n","\n","In this tutorial, we will do the following:\n","1. Load and process the data\n","2. Understand the data using exploratory data analysis\n","3. Train a machine learning model using an open sourced software package called Prophet\n","4. Save the final machine learning model\n","5. Load the machine learning model for scoring and making predictions\n","\n","## Prerequisites\n","- Have a lakehouse added to this notebook. We will be downloading data from a public blob, and storing that in the lakehouse. "],"metadata":{},"id":"3a09641f-699c-415a-9a84-2cd3bc91e886"},{"cell_type":"markdown","source":["## Introduction \n","\n","We will use [NYC Property Sales data](https://www1.nyc.gov/site/finance/about/open-portal.page) range from 2003 to 2015 published by NYC Department of Finance on the [NYC Open Data Portal](https://opendata.cityofnewyork.us/). \n","\n","The dataset is a record of every building sold in New York City property market during 13-year period. Please refer to [Glossary of Terms for Property Sales Files](https://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf) for definition of columns in the spreadsheet. Two example rows are shown below:\n","\n","|borouge|neighborhood|building_class_category|tax_class|block|lot|eastment|building_class_at_present|address|apartment_number|zip_code|residential_units|commercial_units|total_units|land_square_feet|gross_square_feet|year_built|tax_class_at_time_of_sale|building_class_at_time_of_sale|sale_price|sale_date|\n","|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|\n","|Manhattan|ALPHABET CITY|07  RENTALS - WALKUP APARTMENTS|0.0|384.0|17.0||C4|225 EAST 2ND   STREET||10009.0|10.0|0.0|10.0|2145.0|6670.0|1900.0|2.0|C4|275000.0|2007-06-19|\n","|Manhattan|ALPHABET CITY|07  RENTALS - WALKUP APARTMENTS|2.0|405.0|12.0||C7|508 EAST 12TH   STREET||10009.0|28.0|2.0|30.0|3872.0|15428.0|1930.0|2.0|C7|7794005.0|2007-05-21|\n","\n","We will build a model that forcasts the monthly total sales based on historic data. We will use [Facebook Prophet](https://facebook.github.io/prophet/) to do the forecasting. \n","\n","Prophet is an open-source forecasting library developed by Facebook. Prophet is useful for analyzing time series data and making predictions based on historical trends and seasonal patterns. It is designed to be robust to outliers and missing values, and to be user friendly. \n","\n","Prophet uses a decomposable time series model which consist of three components: **trend**, **seasonality** and **holidays**. \n","- For the **trend** part, Prophet assumes piece-wise constant rate of growth with automatic change point selection.\n","- For **seasonality** part, Prophet models weekly and yearly seasonality using Fourier Series. \n","- We will be aggregating our data on a monthly level, and therefore will not be considering **holidays**. \n","\n","For more information about the modeling techniques used by Prophet, you can read their paper [here](https://peerj.com/preprints/3190/)."],"metadata":{},"id":"13734e31-5030-4cdd-a110-b0f5cf93d663"},{"cell_type":"markdown","source":["### Install Prophet\n","\n","Before we get started we'll need to install Prophet."],"metadata":{},"id":"a45f1380-0281-47dc-bef9-ca810e87ebcd"},{"cell_type":"code","source":["%pip install prophet"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"finished","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.185159Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.8437809Z","spark_jobs":null,"parent_msg_id":"9d63b5c3-5369-4fb2-abcd-98855002584d"},"text/plain":"StatementMeta(, , , Finished, )"},"metadata":{}},{"output_type":"error","ename":"LivyHttpRequestFailure","evalue":"[InternalServerError] Internal server error encountered while preparing service authentication tokens. HTTP status code: 500.","traceback":["LivyHttpRequestFailure: [InternalServerError] Internal server error encountered while preparing service authentication tokens. HTTP status code: 500."]}],"execution_count":1,"metadata":{"jupyter":{"outputs_hidden":true},"cellStatus":"{}"},"id":"2acc2475-58f7-48f9-a30d-4f3717e7e2dd"},{"cell_type":"markdown","source":["## Step 1: Load the Data"],"metadata":{},"id":"d9395440-69b0-4971-99a6-14f7c0b3ccac"},{"cell_type":"markdown","source":["#### Download dataset and Upload to Lakehouse\n","\n","There are 15 csv files containig property sales records from 5 boroughs in New York since 2003 to 2015. For your convenience, these files are compressed in `nyc_property_sales.tar` and is available in a public blob storage."],"metadata":{},"id":"9ae6fede-3904-4a96-9eb6-ec83782c2e9d"},{"cell_type":"code","source":["URL = \"https://synapseaisolutionsa.blob.core.windows.net/public/NYC_Property_Sales_Dataset/\"\n","TAR_FILE_NAME = \"nyc_property_sales.tar\"\n","DATA_FOLDER = \"Files/NYC_Property_Sales_Dataset\"\n","TAR_FILE_PATH = f\"/lakehouse/default/{DATA_FOLDER}/tar/\"\n","CSV_FILE_PATH = f\"/lakehouse/default/{DATA_FOLDER}/csv/\"\n","\n","EXPERIMENT_NAME = \"aisample-timeseries\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1859091Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9129137Z","spark_jobs":null,"parent_msg_id":"250648e5-c31b-4e83-ab4e-e5cd45eef452"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"0f56f7c2-92f5-4fa7-8abc-82393d223fee"},{"cell_type":"markdown","source":["**Note:** if you have not added a lakehouse to the notebook, you will be met with an error below. \n","\n","If you have added a lakehouse, then we will be downloading the data from the URL specified above, and storing it in the lakehouse. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8be3591d-56c0-48b8-8d4e-f19cd6b305e7"},{"cell_type":"code","source":["import os\n","\n","if not os.path.exists(\"/lakehouse/default\"):\n","    # ask user to add a lakehouse if no default lakehouse added to the notebook.\n","    # a new notebook will not link to any lakehouse by default.\n","    raise FileNotFoundError(\n","        \"Default lakehouse not found, please add a lakehouse for the notebook.\"\n","    )\n","else:\n","    # check if the needed files are already in the lakehouse, try to download and unzip if not.\n","    if not os.path.exists(f\"{TAR_FILE_PATH}{TAR_FILE_NAME}\"):\n","        os.makedirs(TAR_FILE_PATH, exist_ok=True)\n","        os.system(f\"wget {URL}{TAR_FILE_NAME} -O {TAR_FILE_PATH}{TAR_FILE_NAME}\")\n","\n","    os.makedirs(CSV_FILE_PATH, exist_ok=True)\n","    os.system(f\"tar -zxvf {TAR_FILE_PATH}{TAR_FILE_NAME} -C {CSV_FILE_PATH}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1875749Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9146834Z","spark_jobs":null,"parent_msg_id":"596f5973-7f8a-4f71-b752-5cce8fee3550"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"cellStatus":"{}"},"id":"0b8c0a0b-efed-41b8-8a65-4bb2f0f73316"},{"cell_type":"code","source":["# to record the notebook running time\n","import time\n","\n","ts = time.time()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.188187Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9149887Z","spark_jobs":null,"parent_msg_id":"43748a48-7b01-4e2c-92b0-5879d7a5d18d"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{}"},"id":"9fbfea09-fdcb-4a50-9dda-48e124569ea6"},{"cell_type":"code","source":["# setup mlflow experiment\n","import mlflow\n","\n","mlflow.set_experiment(EXPERIMENT_NAME)\n","mlflow.autolog(disable=True)  # disable mlflow autologging"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1887244Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9154903Z","spark_jobs":null,"parent_msg_id":"8a949aa5-3a90-49df-8110-f95d834c621b"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"b6f0bef2-f1fe-43f7-bb0b-19033e9dbd38"},{"cell_type":"markdown","source":["#### Create Dataframe from Lakehouse\n","\n","First lets see if the data is what we are expecting. It is recommended to manually go through a subset of your data whenever you are working on a new dataset. This practise will give you a better understanding of your data. Even looking at a few dozen rows can quickly teach you a lot about your data.\n","\n","The `display` function prints the dataframe. You can also show the \"Chart\" views to easily visualize a subset of the data"],"metadata":{},"id":"d591b39b-875b-4067-8c6f-b77fe1686814"},{"cell_type":"code","source":["df = (\n","    spark.read.format(\"csv\")\n","    .option(\"header\", \"true\")\n","    .load(\"Files/NYC_Property_Sales_Dataset/csv\")\n",")\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1895819Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9156612Z","spark_jobs":null,"parent_msg_id":"905008b1-d3f8-4af8-b967-95b54adcd058"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"ee537329-2ece-40cb-9035-d179f3109184"},{"cell_type":"markdown","source":["## Step 2: Exploratory Data Analysis"],"metadata":{},"id":"360deb3f-a194-45b1-a199-944508d8e5f7"},{"cell_type":"markdown","source":["Some of the observations based on manually going through the data include:\n","- The sale price is sometimes $0. According to the [Glossary of Terms](https://www.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf), this implies a transfer of ownership without a cash consideration. **We should remove these cases where `sales_price` is 0 from our dataset.**\n","- There are a few different building classes included in this dataset. For our problem we want to focus only on residential buildings. According to the glossary of terms, this means building classes of type \"A\". **We should filter our data to only residential buildings.** We have two options for this; `building_class_at_time_of_sale` or `building_class_at_present`. We'll go for the building class at time of sale as it captures what the building was sold as. \n","- There are some situations where the `total_units` is 0, or the `gross_square_feet` is 0. **Lets remove all instances where either `total_units` or `gross_square_units` are 0.**\n","- Some other columns have missing or NULL values, like the `apartment_number`, `tax_class`, `build_class_at_present`. It seems reasonable that this missing data is due to clerical errors or because the data doesn't exist (e.g. there is no apartment number for the unit). None of these missing values are important to our analysis, so we will ignore these missing values.\n","- The `sale_price` is stored as a string with \"$\" prepended to it. If we want to do analysis on this, this information is better represented as a number. We could feasibly store this as a float, but the raw data doesn't let us retrieve any information after the decimal. **So lets cast the `sale_price` column to an integer.**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"308a8259-fce9-4d7a-b386-1c0a87610858"},{"cell_type":"markdown","source":["#### Type Conversion and Filtering\n","Lets take action on some of the tasks identified above. First we need to do some imports from pyspark."],"metadata":{},"id":"4aa5212e-ca38-4e64-a24b-fcc8be8b7783"},{"cell_type":"code","source":["# import libs\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1902906Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.915816Z","spark_jobs":null,"parent_msg_id":"cfc48fa5-2bf2-4076-977b-92e8302ec8af"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"e5567b6d-6870-49e6-a3d9-487940ded947"},{"cell_type":"markdown","source":["First lets cast our sales data from a string to an integer. \\\n","We use regular expressions to split the numeric portion of the string from the dollar sign (i.e. splitting \"$\" and \"300,000\", in the string \"$300,000\"), and then we cast the numeric portion to an integer.\n","\n","Secondly, lets filter our data to only include situations where all the below conditions are true:\n","1. the sales price is greater than 0\n","2. the total_units is greater than 0\n","3. the gross_square_feet is greater than 0\n","4. the building class is of type A"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"54768951-0e23-4700-b7ed-a48afe1ec9c5"},{"cell_type":"code","source":["df = df.withColumn(\n","    \"sale_price\", F.regexp_replace(\"sale_price\", \"[$,]\", \"\").cast(IntegerType())\n",")\n","df = df.select(\"*\").where(\n","    'sale_price > 0 and total_units > 0 and gross_square_feet > 0 and building_class_at_time_of_sale like \"A%\"'\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1918014Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9160252Z","spark_jobs":null,"parent_msg_id":"f75d41e6-70d4-4326-bf87-74f9c075d88f"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"f5496969-1c4c-4060-ac0e-84262760a122"},{"cell_type":"markdown","source":["#### Aggregating on a monthly basis\n","\n","The sales of property is tracked on a daily level, but this is too granular for our purpose. We should aggregate our data on a monthly basis. \n","\n","First lets change our dates to only show years and months.\n","Note that the data still contains the year so that we can distinguish the month of December in 2005 from the month of December in 2006.\n","\n","Also, lets keep just the columns we care about: `sales_price`, `total_units`, `gross_square_feet` and the `sales_date` which we'll rename to `month`."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"210e9c9a-51af-47e9-8b08-288d2f034afc"},{"cell_type":"code","source":["monthly_sale_df = df.select(\n","    \"sale_price\",\n","    \"total_units\",\n","    \"gross_square_feet\",\n","    F.date_format(\"sale_date\", \"yyyy-MM\").alias(\"month\"),\n",")\n","\n","display(monthly_sale_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1924562Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9161922Z","spark_jobs":null,"parent_msg_id":"8c20da15-6834-4804-b636-1e165a080182"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"fdc8efb3-e463-4fbc-a3dd-9e603d50891a"},{"cell_type":"markdown","source":["Next, lets aggregate the `sale_price`, `total_units` and `gross_square_feet` by month. \n","\n","We will group the data by `month`, and sum all values within the group. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6f717c7a-c788-4b07-961c-462817931433"},{"cell_type":"code","source":["summary_df = (\n","    monthly_sale_df.groupBy(\"month\")\n","    .agg(\n","        F.sum(\"sale_price\").alias(\"total_sales\"),\n","        F.sum(\"total_units\").alias(\"units\"),\n","        F.sum(\"gross_square_feet\").alias(\"square_feet\"),\n","    )\n","    .orderBy(\"month\")\n",")\n","\n","display(summary_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1930895Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.916346Z","spark_jobs":null,"parent_msg_id":"a403e68b-b56e-440a-9a47-b1edb9877d94"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"f0262246-766f-4f5b-8ab6-61cc640416cc"},{"cell_type":"markdown","source":["### From pyspark dataframes to pandas dataframes\n","Pyspark dataframes are great when operating with large datasets. Because we have now aggregated the data, the size of the dataframe is much smaller. This means we can now operate with pandas dataframes which you may be more familiar with.\n","\n","Casting a model from a pyspark dataframe to a pandas dataframe is easy as shown below."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7e1e46eb-9b18-4451-bb25-83245a9b2aae"},{"cell_type":"code","source":["import pandas as pd\n","\n","df_pandas = summary_df.toPandas()\n","display(df_pandas)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1940243Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9164986Z","spark_jobs":null,"parent_msg_id":"9c9576c5-bfab-432f-97b3-1da8600e0daa"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"d4be082f-9243-472d-84e4-f9d8e3aaae14"},{"cell_type":"markdown","source":["### Visualization\n","Now, let's take a look at the trend of property trade trend at NYC. This allows us to quickly get a feel for the patterns and seasonality that might exist. "],"metadata":{},"id":"2dce216c-7d2a-4002-8e8a-f01dea456f10"},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","f, (ax1, ax2) = plt.subplots(2, 1, figsize=(35, 10))\n","plt.sca(ax1)\n","plt.xticks(np.arange(0, 15 * 12, step=12))\n","plt.ticklabel_format(style=\"plain\", axis=\"y\")\n","sns.lineplot(x=\"month\", y=\"total_sales\", data=df_pandas)\n","plt.ylabel(\"Total Sales\")\n","plt.xlabel(\"Time\")\n","plt.title(\"Total Property Sales by Month\")\n","\n","plt.sca(ax2)\n","plt.xticks(np.arange(0, 15 * 12, step=12))\n","plt.ticklabel_format(style=\"plain\", axis=\"y\")\n","sns.lineplot(x=\"month\", y=\"square_feet\", data=df_pandas)\n","plt.ylabel(\"Total Square Feet\")\n","plt.xlabel(\"Time\")\n","plt.title(\"Total Property Square Feet Sold by Month\")\n","plt.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1952514Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9166491Z","spark_jobs":null,"parent_msg_id":"0a085d43-7868-4d0e-a100-e7d09815c85e"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"f5f0a234-1e2a-49b5-9144-e99251f06f33"},{"cell_type":"markdown","source":["Some observations we can make:\n","- There is a clear recurring pattern on a yearly cadence. I.e. there is **yearly seasonality**. It seems like the summer months see higher sales volumes than the winter months.\n","- In years with high sales the difference between high sales months and low sales months is greater in absolute terms, than in years where the sales is low. For example, in 2004, the difference between the highest month and the the lowest month is about $900,000,000 - $500,000,000 = $400,000,000. But in 2011 the difference is only $400,000,000 - $300,000,000 = $100,000,000. This becomes important later when we have to decide between **multiplicative** vs. **additive** seasonality effects. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2efce85d-ff13-4898-b464-cd2105a42d96"},{"cell_type":"markdown","source":["### Summary of steps so far\n","- We filtered out non-cash transfers of property, as this would impact our modeling.\n","- We removed untrustworthy datapoints which showed 0 square footage, or 0 units sold.\n","- We aggregated the data over months, rather than days.\n","- We noted a clear yearly seasonality in the data.\n","- It seems like the seasonality in the data is multiplicative rather than additive."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"60407659-fb2c-4cdf-8c7c-3ed7a636f905"},{"cell_type":"markdown","source":["## Step 3: Model Training and Evaluation"],"metadata":{},"id":"651b362d-c9c2-4d3a-a672-fb996377c45c"},{"cell_type":"markdown","source":["#### Model fitting\n","\n","We will now be doing model fitting.\n","\n","The input to Prophet is always a dataframe with two columns; a time axis `ds` and a value axis `y`. The time column should be in a format like YYYY_MM, as it is for us already. The value column must be a numeric.\n","\n","So to do model fitting, we just need to rename the time axis to `ds` and value axis to `y` and pass the data to Prophet.\n","\n","For more information on this see [Prophet's documentation](https://facebook.github.io/prophet/docs/quick_start.html#python-api)."],"metadata":{},"id":"0e4d8557-8e95-4a20-aba7-ef7fa97de9aa"},{"cell_type":"code","source":["df_pandas[\"ds\"] = pd.to_datetime(df_pandas[\"month\"])\n","df_pandas[\"y\"] = df_pandas[\"total_sales\"]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1959253Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9167985Z","spark_jobs":null,"parent_msg_id":"f8b46f80-c107-48f4-a683-695ceb44e1b8"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"0a728444-4b39-452f-ab95-8b09c6566264"},{"cell_type":"markdown","source":["Now let's fit the model.\n","\n","Prophet follows the same convention as sklearn. First you create a new instance of Prophet with some parameters (such as seasonality_mode), and then you fit that instance with data.\n","\n","\n","- We will use **'multiplicative' seasonality**, instead of a constant additive factor which is what Prophet uses by default. This directly came from our analysis in the previous section. Because the amplitude of our seasonality changes, using a simple additive seasonality won't fit the data well.\n","\n","- We will also turn **weekly_seasonality off**, as we do not have weekly data; we have aggregated the data by month.\n","\n","- We will use **Markov Chain Monte Carlo (MCMC)**. By default Prophet will be able to provide uncertainty estimates on the trend and observation noise, but does not provide uncertainty estimates for seasonality. Using MCMC takes longer, but does allow the algorithm to provide uncertainty estimates on the seasonality in addition to the trend and observation noise uncertainty estimates. For more info, refer to [Prophets documentation](https://facebook.github.io/prophet/docs/uncertainty_intervals.html).\n","\n","- Finally, the we will play with the sensitivity of automatic changepoint detection through the **changepoint_prior_scale parameter**. The Prophet algorithm automatically tries to find places in the data where the trajectories abruptly change. Its hard to know what the right value is for this, so we will try a few options and select the best performing model. More information on this parameter in [Prophet's documentation](https://facebook.github.io/prophet/docs/trend_changepoints.html)."],"metadata":{},"id":"22571e90-e947-45f3-a574-772df43a2488"},{"cell_type":"code","source":["from prophet import Prophet\n","from prophet.plot import add_changepoints_to_plot\n","\n","models = []\n","\n","seasonality_mode = \"multiplicative\"\n","weekly_seasonality = False\n","changepoint_priors = [0.01, 0.05, 0.1]\n","mcmc_samples = 100\n","\n","for chpt_prior in changepoint_priors:\n","    m = Prophet(\n","        seasonality_mode=seasonality_mode, weekly_seasonality=weekly_seasonality, changepoint_prior_scale = chpt_prior, mcmc_samples = mcmc_samples\n","    )\n","    m.fit(df_pandas)\n","    models.append(m)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1965821Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9170044Z","spark_jobs":null,"parent_msg_id":"e91204e4-1aed-4741-a3cc-766cd6ecc0d4"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"cellStatus":"{}"},"id":"5ccb0fff-f9f3-4d00-9140-4a1452417ec5"},{"cell_type":"markdown","source":["#### Visualize the model with Prophet\n","Prophet has built-in visualization functions. Lets use these functions to show the model fitting results. \n","\n","The black dots are data points used to train the model. The blue line is the prediction, and the light blue area shows uncertainty intervals.\n","\n","We have built three models with varying levels of changepoint_prior_scale. The predictions of the three models are shown below:"],"metadata":{},"id":"07cc4d0d-af64-4675-afe8-1680e5f6f138"},{"cell_type":"code","source":["for idx, m in enumerate(models):\n","    future = m.make_future_dataframe(periods=12, freq=\"M\")\n","    forecast = m.predict(future)\n","    fig = m.plot(forecast)\n","    fig.suptitle(f\"changepoint = {changepoint_priors[idx]}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1972338Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9171684Z","spark_jobs":null,"parent_msg_id":"b61a7aa3-9c68-4d54-9eda-e334f24b4bc4"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{}"},"id":"cc0dd4e0-e9a1-43df-a3f2-3bacf069965a"},{"cell_type":"markdown","source":["#### Visualize trend and seasonality with Prophet\n","\n","One useful feature of Prophet is that you can easily visualize the underlying trends and seasonalities. Here the light blue area reflects uncertainty.\n","\n","There appears to be a strong long-period oscillating trend; over the course of a few years the sales volumes rises and falls. "],"metadata":{},"id":"6fb9e1e8-995b-4bbf-ba82-7522795771e7"},{"cell_type":"code","source":["fig2 = models[1].plot_components(forecast)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1979208Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9173152Z","spark_jobs":null,"parent_msg_id":"9c1a1b16-9f34-4980-baa8-1bdca334526c"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"c1c77d4b-9b32-44b8-b462-8d25a144c37d"},{"cell_type":"markdown","source":["#### Cross Validation\n","Prophet has a built-in cross-validation tool. This can be useful to estimate the forecast error and helps us find the best performing model.\n","\n","Normally when doing cross-validation, we randomly sample the data and put some in a training set and the rest in a validation test. However, this approach does not work for time-series data. If the model has seen January-2005, and March-2005, and we try to predict February-2005, the model can sort of \"cheat\" - it has seen where the data is going. In real life you are forecasting into the _future_, into unseen regions. \n","\n","The way to make this a fair test is to split the data on dates; you use data up to some date (say the first 11 years of data), and then predict on the remaining, unseen data.\n","\n","Prophet makes that easy. **Below we start with 11 years of training data**, and then make monthly predictions with 1 year horizon. We compare these **predictions with the real world values** to establish how good our model is at predictions.\n","\n","So our training data contains everything from 2003-2013. Then our first run is to predict Jan-2014 through Jan-2015. Then we predict Feb-2014 through Feb-2015, and so on. \n","\n","We repeat this for every one of our three models so we can compare which model performed the best."],"metadata":{},"id":"189b4155-1d92-4b29-b07c-6b4bd68c25c5"},{"cell_type":"code","source":["from prophet.diagnostics import cross_validation\n","from prophet.diagnostics import performance_metrics\n","\n","df_metrics = []\n","for m in models:\n","    df_cv = cross_validation(m, initial=\"11 Y\", period=\"30 days\", horizon=\"365 days\")\n","    df_p = performance_metrics(df_cv, monthly=True)\n","    df_metrics.append(df_p)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.1986493Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9174674Z","spark_jobs":null,"parent_msg_id":"278a9d88-0a55-4542-9c7e-ded842961e60"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"cellStatus":"{}"},"id":"288180dd-5fc2-4e06-ae38-4da31bb1797d"},{"cell_type":"markdown","source":["We can see the various metrics, like MSE, for each of the models by displaying them as follows. Note the varying horizons: we predict one year in the future 12 times. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"46843a36-ae8a-4dde-b2c2-c2b1d41bbd5d"},{"cell_type":"code","source":["display(df_metrics[0])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.2036161Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9176151Z","spark_jobs":null,"parent_msg_id":"4a506901-50de-4816-a138-0dd3a3c0ffaa"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"429cca09-1b75-4ba4-9e70-0f140fb8c3e7"},{"cell_type":"markdown","source":["## Step 4: Register final ML Model\n","\n","We should log these models so that we can remember what parameters we have tried, and we should save the models for later use. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"00540325-9e5d-426a-bef6-e78f49ce8948"},{"cell_type":"markdown","source":["#### Log Model with MLFlow\n","Now we can save the trained models for later use. Here we use mlflow to log metrics/models."],"metadata":{},"id":"88ef1227-d4ca-42d7-81e4-6784dd19c6f8"},{"cell_type":"code","source":["# setup mlflow\n","from mlflow.models.signature import infer_signature\n","\n","signature = infer_signature(future, forecast)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.2044071Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9177632Z","spark_jobs":null,"parent_msg_id":"1f6c3ff6-5fb0-43a4-b757-0387ef712216"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"fd9602a4-0d63-4c73-bbe2-415912aacb9c"},{"cell_type":"code","source":["# log model, metrics and params\n","model_name = f\"{EXPERIMENT_NAME}-prophet\"\n","\n","for idx, m in enumerate(models):\n","    with mlflow.start_run() as run:\n","        mlflow.prophet.log_model(\n","            m, model_name, registered_model_name=model_name, signature=signature\n","        )\n","        mlflow.log_params(\n","            {\"seasonality_mode\":  seasonality_mode,\n","             \"mcmc_samples\": mcmc_samples,\n","             \"weekly_seasonality\": weekly_seasonality,\n","             \"changepoint_prior\": changepoint_priors[idx]}\n","        )\n","        metrics = df_metrics[idx].mean().to_dict()\n","        metrics.pop(\"horizon\")\n","        mlflow.log_metrics(metrics)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.2051859Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-08-23T12:24:49.9179703Z","spark_jobs":null,"parent_msg_id":"b37daa18-86a6-474a-a652-c8f4f823f4e1"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"cellStatus":"{}"},"id":"fb2d5d5e-abc9-4937-916e-3be3c9bbedb2"},{"cell_type":"markdown","source":["## Step 5: Save prediction results\n","Lastly, we'll deploy the model and save the prediction results."],"metadata":{},"id":"63646ec6-ca06-4398-8984-32b199db8f5b"},{"cell_type":"markdown","source":["#### Prediction with *Predict* Transformer\n","\n","We can now load the model and use it to make predictions. You can learn more about ```PREDICT``` and how to use it within Fabric [here](https://aka.ms/fabric-predict)."],"metadata":{},"id":"3298f2f1-f740-4873-acb3-65ab7823ca12"},{"cell_type":"code","source":["from synapse.ml.predict import MLFlowTransformer\n","\n","spark.conf.set(\"spark.synapse.ml.predict.enabled\", \"true\")\n","\n","model = MLFlowTransformer(\n","    inputCols=future.columns.values,\n","    outputCol=\"prediction\",\n","    modelName=f\"{EXPERIMENT_NAME}-prophet\",\n","    modelVersion=1,\n",")\n","\n","test_spark = spark.createDataFrame(data=future, schema=future.columns.to_list())\n","\n","batch_predictions = model.transform(test_spark)\n","\n","display(batch_predictions)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.2059642Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"88392987-1414-4417-93ee-0d4ae419ec6d"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{}"},"id":"9dae7c78-5f3a-4e4a-b6b4-c5acf4be94fa"},{"cell_type":"code","source":["# code for saving predictions into lakehouse\n","batch_predictions.write.format(\"delta\").mode(\"overwrite\").save(\n","    f\"{DATA_FOLDER}/predictions/batch_predictions\"\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.206731Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"77d68d48-23c9-449a-9ccd-ae3ac1741f58"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{}"},"id":"59f10d8b-f3b1-4e2d-85bb-48544ea4fe52"},{"cell_type":"code","source":["print(f\"Full run cost {int(time.time() - ts)} seconds.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"waiting","livy_statement_state":null,"queued_time":"2023-08-23T12:24:47.2074847Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"54b2b4fc-e128-4156-8f95-0f2b322bd44d"},"text/plain":"StatementMeta(, , , Waiting, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"cellStatus":"{}"},"id":"66426a70-9644-4af4-bbe3-7d528c587980"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}